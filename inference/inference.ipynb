{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4b67d5-d9d7-41ad-aa6a-d196c28b19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372174d-3de4-4520-8569-f4d67695b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "from glob import glob\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import  DataLoader\n",
    "import xarray\n",
    "import pandas as pd\n",
    "from dataset.dataset import ImageDataset, SingleImageDataset, pickle_write, pickle_read\n",
    "from dataset.normalization import  ZeroMinMax\n",
    "from dataset.station_dataset import GroundstationDataset\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from models.LightningModule import LitEstimatorPoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from utils.plotting import prediction_error_plot, plot_station_scatter\n",
    "from cartopy.mpl.gridliner import (\n",
    "    LongitudeFormatter,\n",
    "    LatitudeFormatter,\n",
    "    LongitudeLocator,\n",
    "    LatitudeLocator,\n",
    ")\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "FIGSIZE=(13.33,7.5)\n",
    "DPI=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3ec4f-0743-4a96-acd2-592cc302cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_image(predictions, input_image, config, patch_size):\n",
    "\n",
    "    y_hat = torch.cat([x[0] for x in predictions]).squeeze()\n",
    "    y = torch.cat([x[1] for x in predictions]).squeeze()\n",
    "    lat = torch.cat([x[2][:, 1] for x in predictions])\n",
    "    lon = torch.cat([x[2][:, 2] for x in predictions])\n",
    "\n",
    "    img_dim = (len(input_image.lon) - config.patch_size['x'] +1,\n",
    "               len(input_image.lat) - config.patch_size['y'] +1,)\n",
    "    \n",
    "    y_hat = y_hat.reshape(img_dim)\n",
    "    y = y.reshape(img_dim)\n",
    "    lat = lat.reshape(img_dim)\n",
    "    lon = lon.reshape(img_dim)\n",
    "\n",
    "    y_hat = config.target_transform.inverse(y_hat, [\"SIS\"])\n",
    "    y = config.target_transform.inverse(y, [\"SIS\"])\n",
    "    lat = config.transform.inverse(lat, [\"lat\"])\n",
    "    lon = config.transform.inverse(lon, [\"lon\"])\n",
    "\n",
    "    output_image = xarray.DataArray(\n",
    "        data=y_hat.T,\n",
    "        # dims=('x','y'),\n",
    "        coords={\"lat\": (('lat'), lat[0, :]),\n",
    "                \"lon\": (('lon'), lon[:, 0]),},\n",
    "        attrs=input_image.SIS.attrs,\n",
    "    )\n",
    "\n",
    "    output_image.lat.attrs = input_image.lat.attrs\n",
    "    output_image.lon.attrs = input_image.lon.attrs\n",
    "    return output_image, y, y_hat\n",
    "\n",
    "\n",
    "def image_1d_to_2d(arr, dim, patch_size):\n",
    "    return arr.reshape(\n",
    "        dim[0] - patch_size[\"y\"] + 1,\n",
    "        dim[1] - patch_size[\"x\"] + 1,\n",
    "    )\n",
    "\n",
    "def plot_comparison_image(output_image, ground_truth_image, extent=None, savefig = True, folder='',):\n",
    "    nm = ground_truth_image.name\n",
    "    time = ground_truth_image.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "    time_pretty = ground_truth_image.time.dt.strftime('%Y-%m-%d %H:%M').item()\n",
    "    proj = ccrs.PlateCarree()\n",
    "    # fig, axes = plt.subplots(1,3, sharex=True, sharey=True, figsize=(12,6), subplot_kw={\"projection\": proj})\n",
    "    FIGSIZE=(13.33,7.5)\n",
    "    DPI=120\n",
    "    fig = plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "    gs = fig.add_gridspec(\n",
    "        2,\n",
    "        4,\n",
    "        width_ratios=(3, 3, 3, .2),\n",
    "        height_ratios=(3,.125),\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        bottom=0.1,\n",
    "        top=0.9,\n",
    "        wspace=0.05,\n",
    "        hspace=0.1,\n",
    "    )\n",
    "    ax1 = fig.add_subplot(gs[0, 0], projection=proj)\n",
    "    ax2 = fig.add_subplot(gs[0, 1], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    ax3 = fig.add_subplot(gs[0, 2], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    axc1 = fig.add_subplot(gs[1, :2], autoscale_on=False)\n",
    "    axc2 = fig.add_subplot(gs[:, 3])\n",
    "    \n",
    "    ax_text = fig.add_subplot(gs[1,2])\n",
    "    ax_text.axis('off')\n",
    "    ax_text.annotate(time_pretty, \n",
    "                xycoords='axes fraction',\n",
    "                xy=[.3, .5])\n",
    "\n",
    "    output_image.plot.imshow(x='lon', y='lat', \n",
    "                             vmin=0, vmax=1100, \n",
    "                             ax=ax1, transform=proj, add_colorbar=False)\n",
    "    ground_truth_image.plot.imshow(x='lon', y='lat', \n",
    "                                   vmin=0, vmax=1100, ax=ax2, \n",
    "                                   transform=proj, cbar_ax=axc1,\n",
    "                                  cbar_kwargs={'orientation':'horizontal', 'shrink':0.6, 'aspect':40,'label':'SIS [W/m2]'})\n",
    "    \n",
    "    error = output_image - ground_truth_image.reindex_like(output_image, method=\"nearest\")\n",
    "    error.plot.imshow(ax=ax3, transform=proj, cbar_ax=axc2)\n",
    "    ax1.set_title(\"DL\")\n",
    "    ax2.set_title(\"SARAH3\")\n",
    "    ax3.set_title(\"$\\hat{y} - y$\")\n",
    "    \n",
    "    gls = {}\n",
    "    for i, ax in enumerate([ax1,ax2,ax3]):\n",
    "        ax.coastlines()\n",
    "        gl = ax.gridlines(draw_labels=True, linewidth=.3)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        if i>0:\n",
    "            gl.left_labels = False\n",
    "        \n",
    "    if extent is not None:\n",
    "        ax1.set_xlim([extent[0], extent[2]])\n",
    "        ax1.set_ylim([extent[1], extent[3]])\n",
    "    \n",
    "    if savefig:\n",
    "        if extent is not None:\n",
    "            extent_str = '_'.join([str(x) for x in extent])\n",
    "            fig.savefig(folder  + f'{nm}_comparison_{time}_{extent_str}.png')\n",
    "        else:\n",
    "            pass\n",
    "            fig.savefig(folder + f'{nm}_comparison_{time}.png')\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = R2Score()\n",
    "\n",
    "def statistics_stations(predictions_stations, save = True, plot_text=True, stations_collection='BSRN'):\n",
    "    predictions_stations_new = predictions_stations.copy()\n",
    "    metrics_fn = inference_fn + f'{stations_collection}_metrics.txt'\n",
    "    if save:\n",
    "        with open(metrics_fn, 'w')as f:\n",
    "            f.write('#station;bias;RMSE;MAE;R2\\n')\n",
    "            f.write('#-;w/m2;w/m2;w/m2;-\\n')\n",
    "    for key, val in predictions_stations.items():\n",
    "        y_hat = val['y_hat']\n",
    "        y = val['SIS']\n",
    "        error = y_hat - y\n",
    "        nan = ~error.isnull().compute()\n",
    "        # print(1-nan.sum().item()/error.shape[0], ' ratio of nans')\n",
    "        error = error[nan]\n",
    "        y=y[nan]\n",
    "        y_hat=y_hat[nan]\n",
    "        bias = error.mean().values\n",
    "        mae = abs(error).mean().values\n",
    "        std = error.std().values\n",
    "        median = np.median(error)\n",
    "        rmse = np.sqrt((error**2).mean()).values\n",
    "        R2 = r2_score(y_hat, y)\n",
    "        val.update({'R2':R2, 'RMSE':rmse, 'MAE':mae, 'Bias':bias})    \n",
    "        print(f\"{key}: \\t Bias: {np.round(bias)}\\t RMSE: {np.round(rmse)} \\t MAE: {np.round(mae)} \\t STD: {np.round(std,2)} \\t R2score: {np.round(R2, 3)}\")\n",
    "        fig = prediction_error_plot(torch.tensor(y.values), torch.tensor(y_hat.values), title=key)\n",
    "        if save is True:\n",
    "            with open(metrics_fn, 'a') as f:\n",
    "                f.write(f'{key};{np.round(bias,1)};{np.round(rmse,1)};{np.round(mae,1)};{np.round(R2, 3)}\\n')\n",
    "            fig.savefig(inference_fn + f'prediction_error_plot_{key}.png')\n",
    "    \n",
    "    R2s = [val['R2'] for val in predictions_stations.values()]\n",
    "    rmses = [val['RMSE'] for val in predictions_stations.values()]\n",
    "    maes = [val['MAE'] for val in predictions_stations.values()]\n",
    "    biass = [val['Bias'] for val in predictions_stations.values()]\n",
    "    \n",
    "    rmse_max = np.ceil(np.max(rmses))\n",
    "    maes_max = np.ceil(np.max(maes))\n",
    "    \n",
    "    lats = [val.lat[0].item() for val in predictions_stations.values()]\n",
    "    lons = [val.lon[0].item() for val in predictions_stations.values()]\n",
    "    \n",
    "    fig1 = plot_station_scatter(lats, lons, rmses, predictions_stations.keys(), 'RMSE', vmin=50, vmax=rmse_max if rmse_max > 125 else 125, plot_text=plot_text)\n",
    "\n",
    "    fig2 = plot_station_scatter(lats, lons, maes, predictions_stations.keys(), 'MAE', vmin=30, vmax=maes_max if maes_max > 80 else 80, plot_text=plot_text)\n",
    "\n",
    "    fig3 = plot_station_scatter(lats, lons, biass, predictions_stations.keys(), 'Bias', cmap='bwr', norm=colors.CenteredNorm(), plot_text=plot_text)\n",
    "    if save:\n",
    "        fig1.savefig(inference_fn + f'{stations_collection}_RMSE_plot.png')\n",
    "        fig2.savefig(inference_fn + f'{stations_collection}_MAE_plot.png')\n",
    "        fig3.savefig(inference_fn + f'{stations_collection}_bias_plot.png')\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56199313-8707-4637-bd7c-da8836a7b56b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be720af9-e372-4007-8a42-0aae8e867b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 2048,\n",
    "    \"patch_size\": {\n",
    "        \"x\": 15,\n",
    "        \"y\": 15,\n",
    "        \"stride_x\": 1,\n",
    "        \"stride_y\": 1,\n",
    "    },\n",
    "    \"x_vars\": [\n",
    "        \"channel_1\",\n",
    "        \"channel_2\",\n",
    "        \"channel_3\",\n",
    "        \"channel_4\",\n",
    "        \"channel_5\",\n",
    "        \"channel_6\",\n",
    "        \"channel_7\",\n",
    "        \"channel_8\",\n",
    "        \"channel_9\",\n",
    "        \"channel_10\",\n",
    "        \"channel_11\",\n",
    "        \"DEM\",\n",
    "    ],\n",
    "    \"y_vars\": [\"SIS\"],\n",
    "    \"x_features\": [\"dayofyear\", \"lat\", \"lon\", 'SZA', \"AZI\", 'sat_SZA', 'sat_AZI', 'coscatter_angle'],\n",
    "    \"transform\": ZeroMinMax(),\n",
    "    \"target_transform\": ZeroMinMax(),\n",
    "}\n",
    "config = SimpleNamespace(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de736252-79d3-4fc2-9c88-4cbeae93793e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trainer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb44d4-0de2-43b6-ae75-e6bab09fbcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator = LitEstimator()\n",
    "trainer = Trainer(\n",
    "    # profiler=\"simple\",\n",
    "    # num_sanity_val_steps=2,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"32\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# # chkpt_fn = \"../train/SIS_point_estimation/vh232f2j/checkpoints/epoch=11-step=11928.ckpt\"\n",
    "# # chkpt_fn = '../train/SIS_point_estimation/wv1ykh5d/checkpoints/epoch=8-step=8946.ckpt' # SARAH3 emulator\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/1otg6d0w/checkpoints/epoch=3-val_loss=0.01542.ckpt' # trained on 80 DWD groundstations\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/4krccmyz/checkpoints/epoch=0-val_loss=0.09168.ckpt' # retrained for bias on SARAH3\n",
    "\n",
    "\n",
    "# # chkpt_fn = '../train/SIS_point_estimation/4nbyae30/checkpoints/epoch=7-val_loss=0.01023.ckpt'# SARAH3 emulator\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/pl86of1b/checkpoints/epoch=4-val_loss=0.01630.ckpt' # finetuned on 80 DWD groundstations\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/groundstations_only/checkpoints/epoch=10-val_loss=0.01703.ckpt' # trained only on 80 DWD groundstations\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/ld18qnr7/checkpoints/epoch=19-step=14020.ckpt'# Finetuned for 19 epoch but with parameter loss\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/mps1bagn/checkpoints/epoch=0-val_loss=0.01839.ckpt' # finetuned for 1 epoch with parameter loss\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/wu24vik3/checkpoints/epoch=8-val_loss=0.01883.ckpt' # finetuned on BSRN\n",
    "# # chkpt_fn = '../train/SIS_point_estimation_groundstation/ulw5nuu8/checkpoints/epoch=0-val_loss=0.01874.ckpt' # finetuned for 9 epoch DWD with high parameter loss\n",
    "# chkpt_fn = '../train/SIS_point_estimation_groundstation/drvowqn4/checkpoints/epoch=0-val_loss=0.01659.ckpt' # finetuned DWD, freeze MLP\n",
    "\n",
    "# Emulator \n",
    "# chkpt_fn = '../train/SIS_point_estimation/m4ms61hn/checkpoints/epoch=3-val_loss=0.00304.ckpt'\n",
    "\n",
    "\n",
    "#### TEST coscatterangle \n",
    "# coscatter dwd\n",
    "chkpt_fn = '/scratch/snx3000/kschuurm/irradiance_estimation//train/SIS_point_estimation_groundstation/2oys35k3/checkpoints/epoch=6-step=14469.ckpt' # finetuned DWD, freeze MLP\n",
    "# coscatter bsrn\n",
    "# chkpt_fn = '/scratch/snx3000/kschuurm/irradiance_estimation/train/SIS_point_estimation_groundstation/xqklgx75/checkpoints/epoch=6-step=1274.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "inference_fn = chkpt_fn.split('checkpoints')[0] + 'inference/'\n",
    "if not os.path.exists(inference_fn):\n",
    "    os.mkdir(inference_fn)\n",
    "\n",
    "    \n",
    "estimator = LitEstimatorPoint.load_from_checkpoint(\n",
    "    chkpt_fn,\n",
    "    learning_rate=0.001,\n",
    "    config=config,\n",
    ")\n",
    "print(chkpt_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c77f9-f34e-403c-ba7b-3dffc2d7e38f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1f72c-4d87-4815-855c-29a034353b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_dict(names, datasets):\n",
    "    predictions_stations = [trainer.predict(estimator, DataLoader(ds, 2048, shuffle=False)) for ds in datasets]\n",
    "    \n",
    "    timeindices_l = [ds.timeindices for ds in datasets]\n",
    "    data_l = [ds.data for ds in datasets]\n",
    "    \n",
    "    predictions_dict = { nm: {'y_hat': config.transform.inverse(torch.cat([p[0] for p in pred]), config.y_vars),\n",
    "                              'y': config.transform.inverse(torch.cat([p[1] for p in pred]), config.y_vars),\n",
    "                              'x': config.transform.inverse(torch.cat([p[2] for p in pred]), config.x_features),\n",
    "                             'time': tidx,\n",
    "                             'data':data} for nm, pred, tidx in zip(names, predictions_stations, timeindices_l, data_l)}\n",
    "    \n",
    "    \n",
    "    return predictions_dict\n",
    "\n",
    "def predictions_to_ds(names, datasets):\n",
    "    \n",
    "    pred_ds = {}\n",
    "    for nm, ds, in zip(names, datasets):\n",
    "        data = ds.data\n",
    "        pred = trainer.predict(estimator, DataLoader(ds, 2048, shuffle=False))\n",
    "        \n",
    "        y_hat = config.transform.inverse(torch.cat([p[0] for p in pred]), config.y_vars)\n",
    "        \n",
    "        data['y_hat'] = (('time'), y_hat.squeeze())\n",
    "        pred_ds[nm] = data.drop_vars(['channel_data'])\n",
    "    return pred_ds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0645f-9356-41f0-82d7-779d318ecc39",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BSRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564b59c-18bd-4552-a021-73a524941ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/IEA_PVPS/IEA_PVPS_*.zarr')\n",
    "station_names_bsrn = [os.path.basename(fn).split('IEA_PVPS_')[-1].split('.')[0] for fn in zarr_fns]\n",
    "index = xarray.open_dataset('/scratch/snx3000/kschuurm/DATA/IEA_PVPS/index.nc')\n",
    "\n",
    "\n",
    "a = index.plot.scatter(x='longitude', y='latitude', c='b', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "# index_valid.plot.scatter(x='lon', y='lat', c='r', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "ax = a.axes\n",
    "for i, txt in enumerate(index.station_name):\n",
    "    ax.annotate(txt.values, (index.longitude[i], index.latitude[i]))\n",
    "ax.set_extent([-8, 28, 29, 62])\n",
    "ax.set_xlim([-8, 29])\n",
    "ax.set_ylim([28, 62])\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74defa13-7846-4548-a6e9-4b493922a3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsrn_datasets = [GroundstationDataset(f'../../ZARR/IEA_PVPS/IEA_PVPS_{x}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True)\n",
    "                            for x in tqdm(station_names_bsrn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b362b33-8c8d-40ca-b4d1-92544b0b8688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictions_bsrn = predictions_to_dict(station_names_bsrn, bsrn_datasets)\n",
    "\n",
    "pred_fn = inference_fn + 'BSRN_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_bsrn = predictions_to_ds(station_names_bsrn, bsrn_datasets)\n",
    "    pickle_write(predictions_bsrn, pred_fn)\n",
    "else:\n",
    "    predictions_bsrn = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2de46f9-d279-4f14-afe2-613942162e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_bsrn = pickle_read('BSRN_baseline_predictions.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04159c8f-b080-4332-bb75-8ade49ba07bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_nm = 'TOR'\n",
    "\n",
    "sarahy = baseline_bsrn[station_nm]['y_hat']\n",
    "\n",
    "ds = predictions_bsrn[station_nm]\n",
    "ds['sarahy'] = (('time'), sarahy.squeeze())\n",
    "                               \n",
    "\n",
    "dt =  datetime(2017,7,10,)   \n",
    "\n",
    "fig = plt.figure()\n",
    "ds_sel = ds.sel(time=slice(dt, dt + timedelta(days=1)))\n",
    "lns1 = ds_sel.SIS.plot.line(x='time', label='SIS')\n",
    "lns2 = ds_sel.sarahy.plot.line(x='time', label='SARAH3')\n",
    "lns3 =ds_sel.y_hat.plot.line(x='time', label='Emulator')\n",
    "ax = plt.gca()\n",
    "plt.ylabel('SIS [w/m2]')\n",
    "twinax = ax.twinx()\n",
    "lns4 =ds_sel.KI.plot.line(x='time', label='CSI', c='k', linestyle='--', linewidth=.6,  ax=twinax)\n",
    "twinax.set_ylabel('Clear Sky Index [-]')\n",
    "twinax.set_ylim([0,1.2])\n",
    "# ds_sel.GHIcalc.plot.line(x='time', label='GHIcalc')\n",
    "# ds_sel.DIF.plot.line(x='time', label='DIF')\n",
    "# ds_sel.DNI.plot.line(x='time', label='DNI')\n",
    "\n",
    "lns = lns1+lns2+lns3 + lns4\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs)\n",
    "plt.title(f'{station_nm}   {dt.date()}')\n",
    "\n",
    "folder = '/scratch/snx3000/kschuurm/irradiance_estimation/inference/timeseries_plots/'\n",
    "fig.savefig(folder + f'{station_nm}_{dt.date()}.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0be84f-3dd3-4416-b31e-4c9e65cf8c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chkpt_fn)\n",
    "statistics_stations(predictions_bsrn, save=True, plot_text=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee2053-8b54-401c-9a6d-8ed8d9351014",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34518824-3104-41e4-b077-27dfc171b5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/DWD/DWD_SOLAR_*.zarr')\n",
    "station_names = [int(os.path.basename(fn).split('SOLAR_')[-1].split('.')[0]) for fn in zarr_fns]\n",
    "index = xarray.open_dataset('/scratch/snx3000/kschuurm/DATA/DWD/netcdf/DWD_SOLAR_index.nc')\n",
    "index = index.sel(station_id=station_names)\n",
    "\n",
    "# train_id, valid_id = torch.utils.data.random_split(station_names, [.8, .2])\n",
    "# print(list(train_id), list(valid_id))\n",
    "\n",
    "train_id = [15000, 2638, 662, 342, 691, 4104, 1684, 5426, 1766, 3167, 596, 880, 1346, 4271, 1550, 3196, 5792, 2485, 856, 1468, 3287, 4336, 701, 3126, 891, 1078, 4393, 963, 5705, 5546, 7368, 4887, 164, 704, 2261, 656, 2559, 6197, 3513, 3032, 7351, 430, 1443, 2907, 5856, 5404, 6163, 2483, 3268, 2601, 15444, 13674, 7374, 5480, 7367, 4745, 2014, 4625, 5100, 3761, 460, 7369, 3086, 3366, 282, 591, 1639, 232, 4177, 7370, 2667, 4928, 2712, 4466, 5397, 5516, 1975, 1503, 2115, 1605]\n",
    "valid_id = [1757, 5109, 953, 3028, 2290, 5906, 2171, 427, 2932, 2812, 5839, 1691, 3811, 1420, 5142, 4911, 3660, 3730, 1048]\n",
    "\n",
    "index_train = index.sel(station_id=train_id); index_valid = index.sel(station_id=valid_id)\n",
    "\n",
    "a = index_train.plot.scatter(x='lon', y='lat', c='b', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "index_valid.plot.scatter(x='lon', y='lat', c='r', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "a.axes.gridlines()\n",
    "a.axes.set_extent([-5, 25, 40, 60])\n",
    "ax = a.axes\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "plt.gcf().legend(['train', 'validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f7c64-85e3-4c0f-a5d7-1223363933bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_datasets = [GroundstationDataset(f'../../ZARR/DWD/DWD_SOLAR_{str(x).zfill(5)}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True)\n",
    "                            for x in tqdm(valid_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755fffb-32ab-4474-9e60-9b9623531cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'DWD_valid_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_validation = predictions_to_ds(valid_id, valid_datasets)\n",
    "    pickle_write(predictions_validation, pred_fn)\n",
    "else:\n",
    "    predictions_validation = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634de535-a536-4699-b3e7-77275604311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_validation, True, stations_collection='DWD_valid', plot_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80f129a4-914c-4f33-9e51-7e11c5b7b724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.show()\n",
    "del predictions_validation, valid_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3012c075-8fd2-49cb-bb86-357fe7c7f111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [03:39<00:00,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "train_datasets = [GroundstationDataset(f'../../ZARR/DWD/DWD_SOLAR_{str(x).zfill(5)}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True)\n",
    "                            for x in tqdm(train_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acda5ec-78aa-4950-9e55-c0043be3e246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../train/SIS_point_estimation/m4ms61hn/inference/DWD_train_predictions.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/snx3000/kschuurm/lightning-env/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /scratch/snx3000/kschuurm/lightning-env/lib/python3. ...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/scratch/snx3000/kschuurm/lightning-env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52602c975e14aa4904883dc396d5e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995d14c2756c40efbd4cec7e119c0023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df742fdaf6e49a8ae1c18f1e3352f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017eb16ca01f4fc1ac5734fd4ad7f48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e306137f50ca4e388e31d1746f285fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efd9c0459fe43f09c3004f3bfc7575c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b7080b869f45bf8e2fedc4cb15f8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79c696a16c9452a9673229514a2a30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c15487fe48c4c44a1eb13d0ba6ebd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0456d39b2e50481e95417ae537d2c879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9dd4f52bf547a5830b9691b130966e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ea4be45fd4747b7c72540348db8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0876ff1d74ce47d0bd293213568d1205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbe512fd5ce496886eb1b305a707a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_fn = inference_fn + 'DWD_train_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_train = predictions_to_ds(train_id, train_datasets)\n",
    "    pickle_write(predictions_train, pred_fn)\n",
    "else:\n",
    "    predictions_train = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9face00d-1154-431c-801a-f9fb4be06fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_train, True, stations_collection='DWD_train', plot_text=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae0777-21a5-425a-9124-0e7b9e357f39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KNMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ee276-6d48-41a5-92e8-f3ac77e5eca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/KNMI/KNMI_SOLAR_*.zarr')\n",
    "station_names_knmi = [os.path.basename(fn).split('KNMI_SOLAR_')[-1].split('.')[0] for fn in zarr_fns]\n",
    "index = xarray.open_dataset('/scratch/snx3000/kschuurm/DATA/KNMI/nc/index.nc')\n",
    "index = index.sel(station=station_names_knmi)\n",
    "\n",
    "\n",
    "\n",
    "# index_train = index.sel(station_id=train_id); index_valid = index.sel(station_id=valid_id)\n",
    "\n",
    "a = index.plot.scatter(x='lon', y='lat', c='b', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "# index_valid.plot.scatter(x='lon', y='lat', c='r', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "a.axes.set_extent([0, 10, 50, 55])\n",
    "ax = a.axes\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "plt.gcf().legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d9daa-1d78-4afa-acfb-61077f33f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_knmi = [GroundstationDataset2(f'../../ZARR/KNMI/KNMI_SOLAR_{str(x)}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True)\n",
    "                            for x in tqdm(station_names_knmi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734f7d6-579b-41b7-8ed3-298d9d857c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'KNMI_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_knmi = predictions_to_ds(station_names_knmi, datasets_knmi)\n",
    "    pickle_write(predictions_bsrn, pred_fn)\n",
    "else:\n",
    "    predictions_knmi = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd461e-deab-4bc0-9c8a-dbf43ae04c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_knmi, True, stations_collection='KNMI')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a096c-4b9e-4c6d-bd38-d439cfffd369",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# METEOSWISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a172804-6ef9-499f-8416-6ef5849775aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/METEOSWISS/METEOSWISS_SOLAR_*.zarr')\n",
    "station_names_meteoswiss = [os.path.basename(fn).split('SOLAR_')[-1].split('.')[0] for fn in zarr_fns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4777c-2d14-4933-b560-a60b2c4d25a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets_meteoswisss = [GroundstationDataset2(f'../../ZARR/METEOSWISS/METEOSWISS_SOLAR_{str(x)}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True)\n",
    "                            for x in tqdm(station_names_meteoswiss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acae92-fa9f-4ad9-94e9-9fa52c818f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'METEOSWISS_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_meteoswiss = predictions_to_ds(station_names_meteoswiss, datasets_meteoswisss)\n",
    "    pickle_write(predictions_meteoswiss, pred_fn)\n",
    "else:\n",
    "    predictions_meteoswiss = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36ecc0-7437-43a7-a513-9aa1c8b14941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_meteoswiss, True, stations_collection='METEOSWISS', plot_text = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c987d1-f3ed-4866-988b-691c36719096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Full image comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c25ad99-d28c-4309-9695-2a70765ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDataset = ImageDataset(\n",
    "    x_vars=config.x_vars,\n",
    "    y_vars=config.y_vars,\n",
    "    x_features=config.x_features,\n",
    "    patch_size=config.patch_size,\n",
    "    transform=config.transform,\n",
    "    target_transform=config.target_transform,\n",
    "    batch_in_time=None,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "dts = imageDataset.timeindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88f0e6-e62d-4318-b726-873eb2f0d6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DatetimeIndex(dts)\n",
    "a = a[(a.year == 2021) & (a.month == 2) & (a.month == 2) & (a.day > 11) & (a.day<16)]\n",
    "subset = a.sort_values()\n",
    "print(subset)\n",
    "# snow events: \n",
    "# 11-12 december 2022 around london\n",
    "# 8-9 january 2022 germany\n",
    "# 12-15 feb 2021 germany netherlands\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0bf6e-617a-4c97-be66-d005a956d773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pickle_write(subset, 'SIS_comparison_timindices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c6393-3b51-4019-8ac3-ae3781284aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import valid_test_split, pickle_write, pickle_read\n",
    "\n",
    "# _, subset = valid_test_split(a)\n",
    "# subsubset = np.random.choice(subset, 10, replace=False)\n",
    "# subset = a[::100]\n",
    "subset = pickle_read('SIS_comparison_timindices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70517fea-6c4a-4986-bcee-69849d74ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray_datasets = [xarray.open_dataset(fn) for fn in glob('/scratch/snx3000/kschuurm/DATA/DWD/netcdf/DWD_SOLAR_10min_*.nc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7183552-7095-4b1c-8830-e384ceecfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_ground_truth(output_image, ground_truth_image, point_data=None, extent=None):\n",
    "    nm = ground_truth_image.name\n",
    "    datetime = ground_truth_image.time.values\n",
    "    time = ground_truth_image.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "    time_pretty = ground_truth_image.time.dt.strftime('%Y-%m-%d %H:%M').item()\n",
    "    proj = ccrs.PlateCarree()\n",
    "    # fig, axes = plt.subplots(1,3, sharex=True, sharey=True, figsize=(12,6), subplot_kw={\"projection\": proj})\n",
    "    FIGSIZE=(13.33,7.5)\n",
    "    DPI=120\n",
    "    fig = plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "    gs = fig.add_gridspec(\n",
    "        2,\n",
    "        4,\n",
    "        width_ratios=(3, 3, 3, .2),\n",
    "        height_ratios=(3,.125),\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        bottom=0.1,\n",
    "        top=0.9,\n",
    "        wspace=0.05,\n",
    "        hspace=0.1,\n",
    "    )\n",
    "    ax1 = fig.add_subplot(gs[0, 0], projection=proj)\n",
    "    ax2 = fig.add_subplot(gs[0, 1], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    ax3 = fig.add_subplot(gs[0, 2], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    axc1 = fig.add_subplot(gs[1, :2], autoscale_on=False)\n",
    "    axc2 = fig.add_subplot(gs[:, 3])\n",
    "    \n",
    "    ax_text = fig.add_subplot(gs[1,2])\n",
    "    ax_text.axis('off')\n",
    "    ax_text.annotate(time_pretty, \n",
    "                xycoords='axes fraction',\n",
    "                xy=[.3, .5])\n",
    "    \n",
    "    vmin = 0\n",
    "    vmax = ground_truth_image.max() + 100\n",
    "\n",
    "    output_image.plot.imshow(x='lon', y='lat', \n",
    "                             vmin=0, vmax=vmax, \n",
    "                             ax=ax1, transform=proj, add_colorbar=False)\n",
    "    ground_truth_image.plot.imshow(x='lon', y='lat', \n",
    "                                   vmin=0, vmax=vmax, ax=ax2, \n",
    "                                   transform=proj, cbar_ax=axc1,\n",
    "                                  cbar_kwargs={'orientation':'horizontal', 'shrink':0.6, 'aspect':40,'label':'SIS [W/m2]'})\n",
    "    \n",
    "    norm = colors.TwoSlopeNorm(0, vmin=-100, vmax=100)\n",
    "    dt_10min = datetime - np.timedelta64(10, 'm')\n",
    "    for ds in point_data:\n",
    "        if datetime in ds.time:\n",
    "            ds['GHI'] = ds['GS_10']*1e4/(10*60)\n",
    "            ds = ds.sel(time=datetime)\n",
    "            if isinstance(ds.time.values, np.ndarray):\n",
    "                ds = ds.isel(time=0)\n",
    "            ax3.scatter(ds['lon'], ds['lat'], c=ds['GHI'], vmin=0, vmax=vmax, s=100)\n",
    "            \n",
    "            ghi_gt = ground_truth_image.sel(lat=ds.lat, lon=ds.lon, method='nearest').values\n",
    "            ghi_dl = output_image.sel(lat=ds.lat, lon=ds.lon, method='nearest').values\n",
    "            \n",
    "            ax1.scatter(ds['lon'], ds['lat'], c=ghi_dl - ds['GHI'], s=50, norm=norm, cmap='bwr')\n",
    "            a = ax2.scatter(ds['lon'], ds['lat'], c=ghi_gt - ds['GHI'], s=50, norm=norm, cmap='bwr')\n",
    "    plt.colorbar(a, cax=axc2, label='$\\hat{y} - y$')\n",
    "            \n",
    "            \n",
    "    ax1.set_title(\"DL\")\n",
    "    ax2.set_title(\"SARAH3\")\n",
    "    ax3.set_title(\"Groundstations\")\n",
    "    \n",
    "    gls = {}\n",
    "    for i, ax in enumerate([ax1,ax2,ax3]):\n",
    "        ax.coastlines()\n",
    "        # gl = ax.gridlines(draw_labels=True, linewidth=0)\n",
    "        # gl.top_labels = False\n",
    "        # gl.right_labels = False\n",
    "        # if i>0:\n",
    "        #     gl.left_labels = False\n",
    "        \n",
    "    if extent is not None:\n",
    "        ax1.set_xlim([extent[0], extent[2]])\n",
    "        ax1.set_ylim([extent[1], extent[3]])\n",
    "\n",
    "    return fig\n",
    "        \n",
    "def load_or_predict_data(dt, imageDataset, folder):\n",
    "    fn = folder + f'predictions_{time_str}.pkl'\n",
    "    if not os.path.exists(fn):\n",
    "        dataset = imageDataset.load_singleImageDataset_generator(dt=dt).result()\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=1028, num_workers=20)\n",
    "        predictions = trainer.predict(estimator, dataloader)\n",
    "\n",
    "\n",
    "        output_image, y, y_hat = predictions_to_image(\n",
    "            predictions, dataset.sarah, config, config.patch_size\n",
    "        )\n",
    "        sarah3_SIS = dataset.sarah.SIS.load()\n",
    "        predictions_pkl = {'sarah3':sarah3_SIS, 'dl': output_image}\n",
    "        pickle_write(predictions_pkl, fn)\n",
    "    else:\n",
    "        predictions = pickle_read(fn)\n",
    "        sarah3_SIS = predictions['sarah3']\n",
    "        output_image = predictions['dl']\n",
    "        sarah3_SIS = sarah3_SIS.reindex_like(output_image, method='nearest')\n",
    "        y = sarah3_SIS.values\n",
    "        y_hat = output_image.values\n",
    "    \n",
    "    return output_image, sarah3_SIS, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a6f0b-61f7-4dc2-b5f5-b5a9786e5cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset.dataset import valid_test_split, pickle_write, pickle_read\n",
    "import traceback\n",
    "print(inference_fn)\n",
    "for dt in tqdm(subset):\n",
    "    try:\n",
    "        \n",
    "        folder = inference_fn + 'image_predictions/'\n",
    "        time_str = dt.strftime('%Y%m%d_%H%M')\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        output_image, sarah3_SIS, y, y_hat = load_or_predict_data(dt, imageDataset, folder)\n",
    "        \n",
    "        \n",
    "        plot_comparison_image(output_image, sarah3_SIS, folder = folder)#, extent=[-3,50,2,55])\n",
    "        \n",
    "        extent = [5, 46.5,15,56.5]\n",
    "        fig = plot_with_ground_truth(output_image, sarah3_SIS, xarray_datasets, extent=extent)\n",
    "        extent_str = '_'.join([str(x) for x in extent])\n",
    "        fig.savefig(folder + f'SIS_groundtruth_{time_str}_{extent_str}.png')\n",
    "        \n",
    "        fig = prediction_error_plot(y.flatten(), y_hat.flatten())\n",
    "        fig.savefig(folder + f'prediction_error_plot_{time_str}.png')\n",
    "        \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(dt)\n",
    "        print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7698a5c-42c1-41fa-93ee-b80f44e751e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_with_ground_truth(output_image, sarah3_SIS, xarray_datasets, extent=[5, 46.5,15,56.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d326f9-e66f-46ba-bd60-bc81ab8e7aa6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "from dataset.dataset import pickle_read, pickle_write\n",
    "for dt in tqdm(subset):\n",
    "    try:\n",
    "        dataset = imageDataset.load_singleImageDataset_generator(dt=dt).result()\n",
    "        \n",
    "\n",
    "        time_str = dataset.sarah.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "        folder = inference_fn + 'image_predictions/'\n",
    "        fn = folder + f'predictions_{time_str}.pkl'\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        if not os.path.exists(fn):\n",
    "            print('pickling prediction')\n",
    "            dataloader = DataLoader(dataset, batch_size=2048, num_workers=3)\n",
    "            predictions = trainer.predict(estimator, dataloader)\n",
    "            \n",
    "            \n",
    "            output_image, y, y_hat = predictions_to_image(\n",
    "                predictions, dataset.sarah, config, config.patch_size\n",
    "            )\n",
    "            sarah3_SIS = dataset.sarah.isel(time=0).SIS.load()\n",
    "            predictions_pkl = {'sarah3':sarah3_SIS, 'dl': output_image}\n",
    "            pickle_write(predictions_pkl, fn)\n",
    "        else:\n",
    "            predictions = pickle_read(fn)\n",
    "            sarah3_SIS = predictions['sarah3']\n",
    "            output_image = predictions['dl']\n",
    "            sarah3_SIS = sarah3_SIS.reindex_like(output_image, method='nearest')\n",
    "            y = sarah3_SIS.values\n",
    "            y_hat = output_image.values\n",
    "        \n",
    "#         plot_comparison_image(output_image, sarah3_SIS, folder = folder)#, extent=[-3,50,2,55])\n",
    "\n",
    "#         fig = prediction_error_plot(y.flatten(), y_hat.flatten())\n",
    "#         time_str = dataset.sarah.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "#         fig.savefig(folder + f'prediction_error_plot_{time_str}.png')\n",
    "    except Exception as e:\n",
    "        print(dt)\n",
    "        print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f399d-1996-4203-a938-6e0fab44faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fdb507-c9b8-4009-b42f-b4d8f8739225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = imageDataset.load_singleImageDataset_generator(dt=subset[60]).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b1627-85e5-4672-bd1c-f80d38b3b60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1024, num_workers=20)\n",
    "predictions = trainer.predict(estimator, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26b2c4dd-ba53-4a51-9f98-01a496c59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image, y, y_hat = predictions_to_image(\n",
    "    predictions, dataset.sarah, config, config.patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e69f9-7009-4d85-8766-ae6fbcffbd37",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_comparison_image(output_image, dataset.sarah.SIS)#, extent=[5, 40, 15,50])\n",
    "\n",
    "fig = prediction_error_plot(y.flatten(), y_hat.flatten())\n",
    "time_str = dataset.sarah.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "# fig.savefig(inference_fn + f'prediction_error_plot_{time_str}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffa243-96ac-4dea-937d-eac485a9eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de216744-42ce-4e0a-b284-238efde6909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('/scratch/snx3000/kschuurm/ZARR/SARAH3.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning-kernel2",
   "language": "python",
   "name": "lightning-kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
