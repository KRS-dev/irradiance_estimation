{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b67d5-d9d7-41ad-aa6a-d196c28b19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a3491-3121-42e7-97c0-a2b79fdc1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372174d-3de4-4520-8569-f4d67695b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from types import SimpleNamespace\n",
    "import os\n",
    "from glob import glob\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import  DataLoader\n",
    "import xarray\n",
    "import pandas as pd\n",
    "from dataset.dataset import ImageDataset, pickle_write, pickle_read\n",
    "from dataset.normalization import  ZeroMinMax\n",
    "from dataset.station_dataset import GroundstationDataset\n",
    "from lightning.pytorch import Trainer\n",
    "from models.lightningmodules import LitConvResNet\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils.plotting import prediction_error_plot, plot_station_scatter\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3ec4f-0743-4a96-acd2-592cc302cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_image(predictions, input_image, config, patch_size):\n",
    "\n",
    "    y_hat = torch.cat([x[0] for x in predictions]).squeeze()\n",
    "    y = torch.cat([x[1] for x in predictions]).squeeze()\n",
    "    lat = torch.cat([x[2][:, 1] for x in predictions])\n",
    "    lon = torch.cat([x[2][:, 2] for x in predictions])\n",
    "\n",
    "    img_dim = (len(input_image.lon) - config.patch_size['x'] +1,\n",
    "               len(input_image.lat) - config.patch_size['y'] +1,)\n",
    "    \n",
    "    y_hat = y_hat.reshape(img_dim)\n",
    "    y = y.reshape(img_dim)\n",
    "    lat = lat.reshape(img_dim)\n",
    "    lon = lon.reshape(img_dim)\n",
    "\n",
    "    y_hat = config.target_transform.inverse(y_hat, [\"SIS\"])\n",
    "    y = config.target_transform.inverse(y, [\"SIS\"])\n",
    "    lat = config.transform.inverse(lat, [\"lat\"])\n",
    "    lon = config.transform.inverse(lon, [\"lon\"])\n",
    "\n",
    "    output_image = xarray.DataArray(\n",
    "        data=y_hat.T,\n",
    "        # dims=('x','y'),\n",
    "        coords={\"lat\": (('lat'), lat[0, :]),\n",
    "                \"lon\": (('lon'), lon[:, 0]),},\n",
    "        attrs=input_image.SIS.attrs,\n",
    "    )\n",
    "\n",
    "    output_image.lat.attrs = input_image.lat.attrs\n",
    "    output_image.lon.attrs = input_image.lon.attrs\n",
    "    return output_image, y, y_hat\n",
    "\n",
    "\n",
    "def image_1d_to_2d(arr, dim, patch_size):\n",
    "    return arr.reshape(\n",
    "        dim[0] - patch_size[\"y\"] + 1,\n",
    "        dim[1] - patch_size[\"x\"] + 1,\n",
    "    )\n",
    "\n",
    "def plot_comparison_image(output_image, ground_truth_image, extent=None, savefig = True, folder='',):\n",
    "    nm = ground_truth_image.name\n",
    "    time = ground_truth_image.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "    time_pretty = ground_truth_image.time.dt.strftime('%Y-%m-%d %H:%M').item()\n",
    "    proj = ccrs.PlateCarree()\n",
    "    # fig, axes = plt.subplots(1,3, sharex=True, sharey=True, figsize=(12,6), subplot_kw={\"projection\": proj})\n",
    "    FIGSIZE=(13.33,7.5)\n",
    "    DPI=120\n",
    "    fig = plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "    gs = fig.add_gridspec(\n",
    "        2,\n",
    "        4,\n",
    "        width_ratios=(3, 3, 3, .2),\n",
    "        height_ratios=(3,.125),\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        bottom=0.1,\n",
    "        top=0.9,\n",
    "        wspace=0.05,\n",
    "        hspace=0.1,\n",
    "    )\n",
    "    ax1 = fig.add_subplot(gs[0, 0], projection=proj)\n",
    "    ax2 = fig.add_subplot(gs[0, 1], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    ax3 = fig.add_subplot(gs[0, 2], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    axc1 = fig.add_subplot(gs[1, :2], autoscale_on=False)\n",
    "    axc2 = fig.add_subplot(gs[:, 3])\n",
    "    \n",
    "    ax_text = fig.add_subplot(gs[1,2])\n",
    "    ax_text.axis('off')\n",
    "    ax_text.annotate(time_pretty, \n",
    "                xycoords='axes fraction',\n",
    "                xy=[.3, .5])\n",
    "\n",
    "    output_image.plot.imshow(x='lon', y='lat', \n",
    "                             vmin=0, vmax=1100, \n",
    "                             ax=ax1, transform=proj, add_colorbar=False)\n",
    "    ground_truth_image.plot.imshow(x='lon', y='lat', \n",
    "                                   vmin=0, vmax=1100, ax=ax2, \n",
    "                                   transform=proj, cbar_ax=axc1,\n",
    "                                  cbar_kwargs={'orientation':'horizontal', 'shrink':0.6, 'aspect':40,'label':'SIS [W/m2]'})\n",
    "    \n",
    "    error = output_image - ground_truth_image.reindex_like(output_image, method=\"nearest\")\n",
    "    error.plot.imshow(ax=ax3, transform=proj, cbar_ax=axc2)\n",
    "    ax1.set_title(\"DL\")\n",
    "    ax2.set_title(\"SARAH3\")\n",
    "    ax3.set_title(\"$\\hat{y} - y$\")\n",
    "    \n",
    "    gls = {}\n",
    "    for i, ax in enumerate([ax1,ax2,ax3]):\n",
    "        ax.coastlines()\n",
    "        gl = ax.gridlines(draw_labels=True, linewidth=.3)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        if i>0:\n",
    "            gl.left_labels = False\n",
    "        \n",
    "    if extent is not None:\n",
    "        ax1.set_xlim([extent[0], extent[2]])\n",
    "        ax1.set_ylim([extent[1], extent[3]])\n",
    "    \n",
    "    if savefig:\n",
    "        if extent is not None:\n",
    "            extent_str = '_'.join([str(x) for x in extent])\n",
    "            fig.savefig(folder  + f'{nm}_comparison_{time}_{extent_str}.png')\n",
    "        else:\n",
    "            pass\n",
    "            fig.savefig(folder + f'{nm}_comparison_{time}.png')\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = R2Score()\n",
    "\n",
    "def statistics_stations(predictions_stations, save = True, plot_text=True, stations_collection='IEA_PVPS'):\n",
    "    predictions_stations_new = predictions_stations.copy()\n",
    "    metrics_fn = inference_fn + f'{stations_collection}_metrics.txt'\n",
    "    \n",
    "    if not os.path.exists(inference_fn + f'{stations_collection}/'):\n",
    "        os.mkdir(inference_fn + f'{stations_collection}/')\n",
    "    \n",
    "    if save:\n",
    "        with open(metrics_fn, 'w')as f:\n",
    "            f.write('#station;bias;RMSE;MAE;R2\\n')\n",
    "            f.write('#-;w/m2;w/m2;w/m2;-\\n')\n",
    "    for key in list(predictions_stations.keys()):\n",
    "        val = predictions_stations[key]\n",
    "        y_hat = val['y_hat']\n",
    "        y = val['SIS']\n",
    "        error = y_hat - y\n",
    "        nan = ~error.isnull().compute()\n",
    "        # print(1-nan.sum().item()/error.shape[0], ' ratio of nans')\n",
    "        error = error[nan]\n",
    "        y=y[nan]\n",
    "        y_hat=y_hat[nan]\n",
    "        if not len(y) >0:\n",
    "            print(key, '  only has nans')\n",
    "            del predictions_stations[key]\n",
    "            continue\n",
    "        bias = error.mean().values\n",
    "        mae = abs(error).mean().values\n",
    "        std = error.std().values\n",
    "        median = np.median(error)\n",
    "        rmse = np.sqrt((error**2).mean()).values\n",
    "        R2 = r2_score(y_hat, y)\n",
    "        val.update({'R2':R2, 'RMSE':rmse, 'MAE':mae, 'Bias':bias})    \n",
    "        print(f\"{key}: \\t Bias: {np.round(bias)}\\t RMSE: {np.round(rmse)} \\t MAE: {np.round(mae)} \\t STD: {np.round(std,2)} \\t R2score: {np.round(R2, 3)}\")\n",
    "        fig = prediction_error_plot(torch.tensor(y.values), torch.tensor(y_hat.values), title=key)\n",
    "        if save is True:\n",
    "            with open(metrics_fn, 'a') as f:\n",
    "                f.write(f'{key};{np.round(bias,1)};{np.round(rmse,1)};{np.round(mae,1)};{np.round(R2, 3)}\\n')\n",
    "            fig.savefig(inference_fn + f'{stations_collection}/' +  f'prediction_error_plot_{key}.png')\n",
    "    \n",
    "    R2s = [val['R2'] for val in predictions_stations.values()]\n",
    "    rmses = [val['RMSE'] for val in predictions_stations.values()]\n",
    "    maes = [val['MAE'] for val in predictions_stations.values()]\n",
    "    biass = [val['Bias'] for val in predictions_stations.values()]\n",
    "    \n",
    "    rmse_max = np.ceil(np.max(rmses))\n",
    "    maes_max = np.ceil(np.max(maes))\n",
    "    \n",
    "    lats = [val.lat_station.values for val in predictions_stations.values()]\n",
    "    lons = [val.lon_station.values for val in predictions_stations.values()]\n",
    "    \n",
    "    fig1 = plot_station_scatter(lats, lons, rmses, predictions_stations.keys(), 'RMSE', vmin=50, vmax=rmse_max if rmse_max > 125 else 125, plot_text=plot_text)\n",
    "\n",
    "    fig2 = plot_station_scatter(lats, lons, maes, predictions_stations.keys(), 'MAE', vmin=30, vmax=maes_max if maes_max > 80 else 80, plot_text=plot_text)\n",
    "\n",
    "    fig3 = plot_station_scatter(lats, lons, biass, predictions_stations.keys(), 'Bias', cmap='bwr', norm=colors.CenteredNorm(), plot_text=plot_text)\n",
    "    if save:\n",
    "        fig1.savefig(inference_fn + f'{stations_collection}_RMSE_plot.png')\n",
    "        fig2.savefig(inference_fn + f'{stations_collection}_MAE_plot.png')\n",
    "        fig3.savefig(inference_fn + f'{stations_collection}_bias_plot.png')\n",
    "        \n",
    "\n",
    "def create_GroundstationDataset(*args, **kwargs):\n",
    "    return GroundstationDataset(*args, **kwargs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56199313-8707-4637-bd7c-da8836a7b56b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be720af9-e372-4007-8a42-0aae8e867b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 10000,\n",
    "    \"patch_size\": {\n",
    "        \"x\": 15,\n",
    "        \"y\": 15,\n",
    "        \"stride_x\": 1,\n",
    "        \"stride_y\": 1,\n",
    "    },\n",
    "    \"x_vars\": [\n",
    "        \"channel_1\",\n",
    "        \"channel_2\",\n",
    "        \"channel_3\",\n",
    "        \"channel_4\",\n",
    "        \"channel_5\",\n",
    "        \"channel_6\",\n",
    "        \"channel_7\",\n",
    "        \"channel_8\",\n",
    "        \"channel_9\",\n",
    "        \"channel_10\",\n",
    "        \"channel_11\",\n",
    "        \"DEM\",\n",
    "    ],\n",
    "    \"y_vars\": [\"SIS\"],\n",
    "    \"x_features\": [\"dayofyear\", \"lat\", \"lon\", 'SZA', \"AZI\",], # 'sat_SZA', 'sat_AZI', 'coscatter_angle'],\n",
    "    \"transform\": ZeroMinMax(),\n",
    "    \"target_transform\": ZeroMinMax(),\n",
    "}\n",
    "config = SimpleNamespace(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de736252-79d3-4fc2-9c88-4cbeae93793e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trainer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb44d4-0de2-43b6-ae75-e6bab09fbcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator = LitEstimator()\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    precision=\"32\",\n",
    ")\n",
    "\n",
    "# Emulator \n",
    "# chkpt_fn = '../train/SIS_point_estimation/jup3gn3n/checkpoints/epoch=1-val_loss=0.00705.ckpt'\n",
    "\n",
    "# finetuned\n",
    "# chkpt_fn = '../train/SIS_point_estimation_groundstation/hmobjerd/checkpoints/epoch=1-bsrnval_loss/dataloader_idx_1=0.01771-dwdval_loss/dataloader_idx_0=0.01761.ckpt'\n",
    "# DWD -> METEOSWISS\n",
    "chkpt_fn = '../train/SIS_point_estimation_groundstation/4j9y9tqb/checkpoints/epoch=3-bsrnval_loss/dataloader_idx_1=0.01907-dwdval_loss/dataloader_idx_0=0.01901.ckpt'\n",
    "# DWD -> meteoswiss -> bsrn \n",
    "# chkpt_fn = '../train/SIS_point_estimation_groundstation/hg90jzjy/checkpoints/epoch=1-bsrnval_loss/dataloader_idx_1=0.01735-dwdval_loss/dataloader_idx_0=0.01794-meteoval_loss/dataloader_idx_2=0.02489.ckpt'\n",
    "\n",
    "# groundstations only\n",
    "# chkpt_fn = '../train/SIS_point_estimation_groundstation/6zy2qu70/checkpoints/last.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference_fn = chkpt_fn.split('checkpoints')[0] + 'inference/'\n",
    "if not os.path.exists(inference_fn):\n",
    "    os.mkdir(inference_fn)\n",
    "    os.mkdir(inference_fn + 'IEA_PVPS/')\n",
    "    os.mkdir(inference_fn + 'METEOSWISS/')\n",
    "    os.mkdir(inference_fn + 'DWD_valid/')\n",
    "    os.mkdir(inference_fn + 'DWD_train/')\n",
    "    os.mkdir(inference_fn + 'KNMI/')\n",
    "\n",
    "if not os.path.exists(inference_fn + 'DWD_valid/'):\n",
    "    os.mkdir(inference_fn + 'DWD_valid/')\n",
    "    os.mkdir(inference_fn + 'DWD_train/')\n",
    "    \n",
    "estimator = LitConvResNet.load_from_checkpoint(\n",
    "    chkpt_fn,\n",
    "    learning_rate=0.001,\n",
    "    config=config,\n",
    ")\n",
    "print(chkpt_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c77f9-f34e-403c-ba7b-3dffc2d7e38f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Station dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1f72c-4d87-4815-855c-29a034353b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions_to_dict(names, datasets):\n",
    "    predictions_stations = [trainer.predict(estimator, DataLoader(ds, 20000, shuffle=False)) for ds in datasets]\n",
    "    \n",
    "    timeindices_l = [ds.timeindices for ds in datasets]\n",
    "    data_l = [ds.data for ds in datasets]\n",
    "    \n",
    "    predictions_dict = { nm: {'y_hat': config.transform.inverse(torch.cat([p[0] for p in pred]), config.y_vars),\n",
    "                              'y': config.transform.inverse(torch.cat([p[1] for p in pred]), config.y_vars),\n",
    "                              'x': config.transform.inverse(torch.cat([p[2] for p in pred]), config.x_features),\n",
    "                             'time': tidx,\n",
    "                             'data':data} for nm, pred, tidx, data in zip(names, predictions_stations, timeindices_l, data_l)}\n",
    "    \n",
    "    \n",
    "    return predictions_dict\n",
    "\n",
    "def predictions_to_ds(names, datasets):\n",
    "    \n",
    "    pred_ds = {}\n",
    "    for nm, ds, in zip(names, datasets):\n",
    "        data = ds.data.copy(deep=True)\n",
    "        pred = trainer.predict(estimator, DataLoader(ds, 10000, num_workers=0, shuffle=False))\n",
    "        \n",
    "        y_hat = config.transform.inverse(torch.cat([p[0] for p in pred]), config.y_vars)\n",
    "        \n",
    "        data['y_hat'] = (('time'), y_hat.squeeze())\n",
    "        pred_ds[nm] = data.drop_vars(['channel_data']).load()\n",
    "    return pred_ds\n",
    "\n",
    "def create_GroundstationDataset(*args, **kwargs):\n",
    "    return GroundstationDataset(*args, **kwargs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0645f-9356-41f0-82d7-779d318ecc39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## BSRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564b59c-18bd-4552-a021-73a524941ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/IEA_PVPS/IEA_PVPS_*.zarr')\n",
    "station_names_bsrn = [os.path.basename(fn).split('IEA_PVPS_')[-1].split('.')[0] for fn in zarr_fns]\n",
    "index = xarray.open_dataset('/scratch/snx3000/kschuurm/DATA/IEA_PVPS/index.nc')\n",
    "# index = index.expand_dims(['latitude','longitude']).sel(latitude=slice(29,62), longitude=slice(-8, 28))\n",
    "\n",
    "\n",
    "a = index.plot.scatter(x='longitude', y='latitude', c='b', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "# index_valid.plot.scatter(x='lon', y='lat', c='r', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "ax = a.axes\n",
    "for i, txt in enumerate(index.station_name):\n",
    "    ax.annotate(txt.values, (index.longitude[i], index.latitude[i]))\n",
    "ax.set_extent([-8, 28, 29, 62])\n",
    "ax.set_xlim([-8, 29])\n",
    "ax.set_ylim([28, 62])\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74defa13-7846-4548-a6e9-4b493922a3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsrn_datasets = [GroundstationDataset(f'../../ZARR/IEA_PVPS/IEA_PVPS_{x}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True)\n",
    "                            for x in tqdm(station_names_bsrn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558c73c-7faa-4254-ac1a-17eab334295c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_bsrn = predictions_to_ds(station_names_bsrn, bsrn_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b362b33-8c8d-40ca-b4d1-92544b0b8688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_fn = inference_fn + 'IEA_PVPS_predictions.pkl'\n",
    "print(pred_fn)\n",
    "a =pickle_write(predictions_bsrn, pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9209d-af68-48d9-b60a-36ebe2b6f613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chkpt_fn)\n",
    "statistics_stations(predictions_bsrn, save=True, plot_text=False)\n",
    "plt.show()\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee2053-8b54-401c-9a6d-8ed8d9351014",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34518824-3104-41e4-b077-27dfc171b5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/DWD/DWD_SOLAR_*.zarr')\n",
    "station_names = [int(os.path.basename(fn).split('SOLAR_')[-1].split('.')[0]) for fn in zarr_fns]\n",
    "index = xarray.open_dataset('/scratch/snx3000/kschuurm/DATA/DWD/netcdf/DWD_SOLAR_index.nc')\n",
    "index = index.sel(station_id=station_names)\n",
    "\n",
    "# train_id, valid_id = torch.utils.data.random_split(station_names, [.8, .2])\n",
    "# print(list(train_id), list(valid_id))\n",
    "\n",
    "train_id = [15000, 2638, 662, 342, 691, 4104, 1684, 5426, 1766, 3167, 596, 880, 1346, 4271, 1550, 3196, 5792, 2485, 856, 1468, 3287, 4336, 701, 3126, 891, 1078, 4393, 963, 5705, 5546, 7368, 4887, 164, 704, 2261, 656, 2559, 6197, 3513, 3032, 7351, 430, 1443, 2907, 5856, 5404, 6163, 2483, 3268, 2601, 15444, 13674, 7374, 5480, 7367, 4745, 2014, 4625, 5100, 3761, 460, 7369, 3086, 3366, 282, 591, 1639, 232, 4177, 7370, 2667, 4928, 2712, 4466, 5397, 5516, 1975, 1503, 2115, 1605]\n",
    "valid_id = [1757, 5109, 953, 3028, 2290, 5906, 2171, 427, 2932, 2812, 5839, 1691, 3811, 1420, 5142, 4911, 3660, 3730, 1048]\n",
    "\n",
    "index_train = index.sel(station_id=train_id); index_valid = index.sel(station_id=valid_id)\n",
    "\n",
    "a = index_train.plot.scatter(x='lon', y='lat', c='b', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "index_valid.plot.scatter(x='lon', y='lat', c='r', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "a.axes.gridlines()\n",
    "a.axes.set_extent([-5, 25, 40, 60])\n",
    "ax = a.axes\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "ax.add_feature(cfeature.RIVERS)\n",
    "plt.gcf().legend(['train', 'validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f7c64-85e3-4c0f-a5d7-1223363933bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with ThreadPool(12) as pool:\n",
    "    \n",
    "    valid_datasets = list(tqdm(pool.starmap(create_GroundstationDataset, \n",
    "    zip(\n",
    "        [f'../../ZARR/DWD/DWD_SOLAR_{str(x).zfill(5)}.zarr' for x in valid_id],\n",
    "        repeat(config.y_vars),\n",
    "        repeat(config.x_vars),\n",
    "        repeat(config.x_features),\n",
    "        repeat(config.patch_size['x']),\n",
    "        repeat(config.transform),\n",
    "        repeat(config.target_transform),\n",
    "        repeat(True),\n",
    "    )\n",
    "                                           )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755fffb-32ab-4474-9e60-9b9623531cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'DWD_valid_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_validation = predictions_to_ds(valid_id, valid_datasets)\n",
    "    pickle_write(predictions_validation, pred_fn)\n",
    "else:\n",
    "    predictions_validation = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634de535-a536-4699-b3e7-77275604311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_validation, True, stations_collection='DWD_valid', plot_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012c075-8fd2-49cb-bb86-357fe7c7f111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with ThreadPool(12) as pool:\n",
    "    \n",
    "    train_datasets = pool.starmap(create_GroundstationDataset, \n",
    "    zip(\n",
    "        [f'../../ZARR/DWD/DWD_SOLAR_{str(x).zfill(5)}.zarr' for x in train_id],\n",
    "        repeat(config.y_vars),\n",
    "        repeat(config.x_vars),\n",
    "        repeat(config.x_features),\n",
    "        repeat(config.patch_size['x']),\n",
    "        repeat(config.transform),\n",
    "        repeat(config.target_transform),\n",
    "        repeat(True),\n",
    "    )\n",
    "                                           )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acda5ec-78aa-4950-9e55-c0043be3e246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'DWD_train_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_train = predictions_to_ds(train_id, train_datasets)\n",
    "    pickle_write(predictions_train, pred_fn)\n",
    "else:\n",
    "    predictions_train = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9face00d-1154-431c-801a-f9fb4be06fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_train, True, stations_collection='DWD_train', plot_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ec0ba-ccd0-4acd-bc3c-38c2841429d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae0777-21a5-425a-9124-0e7b9e357f39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KNMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ee276-6d48-41a5-92e8-f3ac77e5eca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray\n",
    "zarr_fns = glob('../../ZARR/KNMI/KNMI_SOLAR_*.zarr')\n",
    "station_names_knmi = [os.path.basename(fn).split('KNMI_SOLAR_')[-1].split('.')[0] for fn in zarr_fns]\n",
    "index = xarray.open_dataset('./DATA/KNMI/nc/index.nc')\n",
    "index = index.sel(station=station_names_knmi)\n",
    "\n",
    "\n",
    "\n",
    "# index_train = index.sel(station_id=train_id); index_valid = index.sel(station_id=valid_id)\n",
    "\n",
    "a = index.plot.scatter(x='lon', y='lat', c='b', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "# index_valid.plot.scatter(x='lon', y='lat', c='r', subplot_kws=dict(projection=ccrs.PlateCarree()), transform=ccrs.PlateCarree())\n",
    "a.axes.set_extent([0, 10, 50, 55])\n",
    "ax = a.axes\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.OCEAN)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "plt.gcf().legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d9daa-1d78-4afa-acfb-61077f33f5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with ThreadPool(12) as pool:\n",
    "    \n",
    "    datasets_knmi = pool.starmap(create_GroundstationDataset, \n",
    "    zip(\n",
    "        [f'../../ZARR/KNMI/KNMI_SOLAR_{str(x)}.zarr' for x in station_names_knmi],\n",
    "        repeat(config.y_vars),\n",
    "        repeat(config.x_vars),\n",
    "        repeat(config.x_features),\n",
    "        repeat(config.patch_size['x']),\n",
    "        repeat(config.transform),\n",
    "        repeat(config.target_transform),\n",
    "        repeat(True),\n",
    "        \n",
    "    ))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734f7d6-579b-41b7-8ed3-298d9d857c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'KNMI_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_knmi = predictions_to_ds(station_names_knmi, datasets_knmi)\n",
    "    pickle_write(predictions_knmi, pred_fn)\n",
    "else:\n",
    "    predictions_knmi = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd461e-deab-4bc0-9c8a-dbf43ae04c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_knmi, True, stations_collection='KNMI', plot_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93aefd6-e436-45db-a524-5aa9c11607a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a096c-4b9e-4c6d-bd38-d439cfffd369",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# METEOSWISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a172804-6ef9-499f-8416-6ef5849775aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_fns = glob('../../ZARR/METEOSWISS/METEOSWISS_SOLAR_*.zarr')\n",
    "station_names_meteoswiss = [os.path.basename(fn).split('SOLAR_')[-1].split('.')[0] for fn in zarr_fns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4777c-2d14-4933-b560-a60b2c4d25a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "with ThreadPool(12) as pool:\n",
    "    \n",
    "    datasets_meteoswisss = list(tqdm(pool.starmap(create_GroundstationDataset, \n",
    "    zip(\n",
    "        [f'../../ZARR/METEOSWISS/METEOSWISS_SOLAR_{str(x)}.zarr' for x in station_names_meteoswiss],\n",
    "        repeat(config.y_vars),\n",
    "        repeat(config.x_vars),\n",
    "        repeat(config.x_features),\n",
    "        repeat(config.patch_size['x']),\n",
    "        repeat(config.transform),\n",
    "        repeat(config.target_transform),\n",
    "        repeat(True),\n",
    "    )\n",
    "                                       )))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acae92-fa9f-4ad9-94e9-9fa52c818f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_fn = inference_fn + 'METEOSWISS_predictions.pkl'\n",
    "print(pred_fn)\n",
    "\n",
    "if not os.path.exists(pred_fn):\n",
    "    predictions_meteoswiss = predictions_to_ds(station_names_meteoswiss, datasets_meteoswisss)\n",
    "    pickle_write(predictions_meteoswiss, pred_fn)\n",
    "else:\n",
    "    print('read pkl')\n",
    "    predictions_meteoswiss = pickle_read(pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36ecc0-7437-43a7-a513-9aa1c8b14941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_stations(predictions_meteoswiss, True, stations_collection='METEOSWISS', plot_text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58432dc6-8b77-4b27-9c45-de33bb817855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25ad99-d28c-4309-9695-2a70765ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDataset = ImageDataset(\n",
    "    x_vars=config.x_vars,\n",
    "    y_vars=config.y_vars,\n",
    "    x_features=config.x_features,\n",
    "    patch_size=config.patch_size,\n",
    "    transform=config.transform,\n",
    "    target_transform=config.target_transform,\n",
    "    batch_in_time=None,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "dts = imageDataset.timeindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88f0e6-e62d-4318-b726-873eb2f0d6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DatetimeIndex(dts)\n",
    "a = a[(a.year == 2021) & (a.month == 2) & (a.month == 2) & (a.day > 11) & (a.day<16) & (a.hour > 10) & (a.hour < 17)]\n",
    "subset = a.sort_values()\n",
    "print(subset)\n",
    "# snow events: \n",
    "# 11-12 december 2022 around london\n",
    "# 8-9 january 2022 germany\n",
    "# 12-15 feb 2021 germany netherlands\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c6393-3b51-4019-8ac3-ae3781284aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import valid_test_split, pickle_write, pickle_read\n",
    "import random\n",
    "\n",
    "\n",
    "# pickle_write(subset, 'SIS_comparison_timindices.pkl')\n",
    "subset = pickle_read('SIS_comparison_timindices.pkl') \n",
    "subset = pd.DatetimeIndex(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70517fea-6c4a-4986-bcee-69849d74ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray_datasets = [xarray.open_dataset(fn) for fn in glob('../DATA/DWD/netcdf/DWD_SOLAR_10min_*.nc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7183552-7095-4b1c-8830-e384ceecfbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_ground_truth(output_image, ground_truth_image, point_data=None, extent=None):\n",
    "    nm = ground_truth_image.name\n",
    "    datetime = ground_truth_image.time.values\n",
    "    time = ground_truth_image.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "    time_pretty = ground_truth_image.time.dt.strftime('%Y-%m-%d %H:%M').item()\n",
    "    proj = ccrs.PlateCarree()\n",
    "    # fig, axes = plt.subplots(1,3, sharex=True, sharey=True, figsize=(12,6), subplot_kw={\"projection\": proj})\n",
    "    FIGSIZE=(13.33,7.5)\n",
    "    DPI=120\n",
    "    fig = plt.figure(figsize=FIGSIZE, dpi=DPI)\n",
    "    gs = fig.add_gridspec(\n",
    "        2,\n",
    "        4,\n",
    "        width_ratios=(3, 3, 3, .2),\n",
    "        height_ratios=(3,.125),\n",
    "        left=0.1,\n",
    "        right=0.9,\n",
    "        bottom=0.1,\n",
    "        top=0.9,\n",
    "        wspace=0.05,\n",
    "        hspace=0.1,\n",
    "    )\n",
    "    ax1 = fig.add_subplot(gs[0, 0], projection=proj)\n",
    "    ax2 = fig.add_subplot(gs[0, 1], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    ax3 = fig.add_subplot(gs[0, 2], projection=proj, sharex=ax1, sharey=ax1)\n",
    "    axc1 = fig.add_subplot(gs[1, :2], autoscale_on=False)\n",
    "    axc2 = fig.add_subplot(gs[:, 3])\n",
    "    \n",
    "    ax_text = fig.add_subplot(gs[1,2])\n",
    "    ax_text.axis('off')\n",
    "    ax_text.annotate(time_pretty, \n",
    "                xycoords='axes fraction',\n",
    "                xy=[.3, .5])\n",
    "    \n",
    "    vmin = 0\n",
    "    vmax = ground_truth_image.max() + 100\n",
    "\n",
    "    output_image.plot.imshow(x='lon', y='lat', \n",
    "                             vmin=0, vmax=vmax, \n",
    "                             ax=ax1, transform=proj, add_colorbar=False)\n",
    "    ground_truth_image.plot.imshow(x='lon', y='lat', \n",
    "                                   vmin=0, vmax=vmax, ax=ax2, \n",
    "                                   transform=proj, cbar_ax=axc1,\n",
    "                                  cbar_kwargs={'orientation':'horizontal', 'shrink':0.6, 'aspect':40,'label':'SIS [W/m2]'})\n",
    "    \n",
    "    norm = colors.TwoSlopeNorm(0, vmin=-100, vmax=100)\n",
    "    dt_10min = datetime - np.timedelta64(10, 'm')\n",
    "    for ds in point_data:\n",
    "        if datetime in ds.time:\n",
    "            ds['GHI'] = ds['GS_10']*1e4/(10*60)\n",
    "            ds = ds.sel(time=datetime)\n",
    "            if isinstance(ds.time.values, np.ndarray):\n",
    "                ds = ds.isel(time=0)\n",
    "            ax3.scatter(ds['lon'], ds['lat'], c=ds['GHI'], vmin=0, vmax=vmax, s=100)\n",
    "            \n",
    "            ghi_gt = ground_truth_image.sel(lat=ds.lat, lon=ds.lon, method='nearest').values\n",
    "            ghi_dl = output_image.sel(lat=ds.lat, lon=ds.lon, method='nearest').values\n",
    "            \n",
    "            ax1.scatter(ds['lon'], ds['lat'], c=ghi_dl - ds['GHI'], s=50, norm=norm, cmap='bwr')\n",
    "            a = ax2.scatter(ds['lon'], ds['lat'], c=ghi_gt - ds['GHI'], s=50, norm=norm, cmap='bwr')\n",
    "    plt.colorbar(a, cax=axc2, label='$\\hat{y} - y$')\n",
    "            \n",
    "            \n",
    "    ax1.set_title(\"DL\")\n",
    "    ax2.set_title(\"SARAH3\")\n",
    "    ax3.set_title(\"Groundstations\")\n",
    "    \n",
    "    gls = {}\n",
    "    for i, ax in enumerate([ax1,ax2,ax3]):\n",
    "        ax.coastlines()\n",
    "        # gl = ax.gridlines(draw_labels=True, linewidth=0)\n",
    "        # gl.top_labels = False\n",
    "        # gl.right_labels = False\n",
    "        # if i>0:\n",
    "        #     gl.left_labels = False\n",
    "        \n",
    "    if extent is not None:\n",
    "        ax1.set_xlim([extent[0], extent[2]])\n",
    "        ax1.set_ylim([extent[1], extent[3]])\n",
    "\n",
    "    return fig\n",
    "        \n",
    "def load_or_predict_data(dt, imageDataset, folder):\n",
    "    fn = folder + f'predictions_{time_str}.pkl'\n",
    "    if not os.path.exists(fn):\n",
    "        dataset = imageDataset.load_singleImageDataset(dt=dt).result()\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=4096, num_workers=24)\n",
    "        predictions = trainer.predict(estimator, dataloader)\n",
    "\n",
    "\n",
    "        output_image, y, y_hat = predictions_to_image(\n",
    "            predictions, dataset.sarah, config, config.patch_size\n",
    "        )\n",
    "        sarah3_SIS = dataset.sarah.SIS.load()\n",
    "        predictions_pkl = {'sarah3':sarah3_SIS, 'dl': output_image}\n",
    "        pickle_write(predictions_pkl, fn)\n",
    "    else:\n",
    "        predictions = pickle_read(fn)\n",
    "        sarah3_SIS = predictions['sarah3']\n",
    "        output_image = predictions['dl']\n",
    "        sarah3_SIS = sarah3_SIS.reindex_like(output_image, method='nearest')\n",
    "        y = sarah3_SIS.values\n",
    "        y_hat = output_image.values\n",
    "    \n",
    "    return output_image, sarah3_SIS, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a6f0b-61f7-4dc2-b5f5-b5a9786e5cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "print(inference_fn)\n",
    "for dt in tqdm(subset):\n",
    "    try:\n",
    "        \n",
    "        folder = inference_fn + 'image_predictions/'\n",
    "        time_str = dt.strftime('%Y%m%d_%H%M')\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        output_image, sarah3_SIS, y, y_hat = load_or_predict_data(dt, imageDataset, folder)\n",
    "        \n",
    "        \n",
    "        plot_comparison_image(output_image, sarah3_SIS, folder = folder)#, extent=[-3,50,2,55])\n",
    "        \n",
    "        extent = [5, 46.5,15,56.5]\n",
    "        fig = plot_with_ground_truth(output_image, sarah3_SIS, xarray_datasets, extent=extent)\n",
    "        extent_str = '_'.join([str(x) for x in extent])\n",
    "        fig.savefig(folder + f'SIS_groundtruth_{time_str}_{extent_str}.png')\n",
    "        \n",
    "        fig = prediction_error_plot(y.flatten(), y_hat.flatten())\n",
    "        fig.savefig(folder + f'prediction_error_plot_{time_str}.png')\n",
    "        \n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(dt)\n",
    "        print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7698a5c-42c1-41fa-93ee-b80f44e751e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_with_ground_truth(output_image, sarah3_SIS, xarray_datasets, extent=[5, 46.5,15,56.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d326f9-e66f-46ba-bd60-bc81ab8e7aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "from dataset.dataset import pickle_read, pickle_write\n",
    "for dt in tqdm(subset):\n",
    "    try:\n",
    "        dataset = imageDataset.load_singleImageDataset_generator(dt=dt).result()\n",
    "        \n",
    "\n",
    "        time_str = dataset.sarah.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "        folder = inference_fn + 'image_predictions/'\n",
    "        fn = folder + f'predictions_{time_str}.pkl'\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        if not os.path.exists(fn):\n",
    "            print('pickling prediction')\n",
    "            dataloader = DataLoader(dataset, batch_size=2048, num_workers=3)\n",
    "            predictions = trainer.predict(estimator, dataloader)\n",
    "            \n",
    "            \n",
    "            output_image, y, y_hat = predictions_to_image(\n",
    "                predictions, dataset.sarah, config, config.patch_size\n",
    "            )\n",
    "            sarah3_SIS = dataset.sarah.isel(time=0).SIS.load()\n",
    "            predictions_pkl = {'sarah3':sarah3_SIS, 'dl': output_image}\n",
    "            pickle_write(predictions_pkl, fn)\n",
    "        else:\n",
    "            predictions = pickle_read(fn)\n",
    "            sarah3_SIS = predictions['sarah3']\n",
    "            output_image = predictions['dl']\n",
    "            sarah3_SIS = sarah3_SIS.reindex_like(output_image, method='nearest')\n",
    "            y = sarah3_SIS.values\n",
    "            y_hat = output_image.values\n",
    "        \n",
    "#         plot_comparison_image(output_image, sarah3_SIS, folder = folder)#, extent=[-3,50,2,55])\n",
    "\n",
    "#         fig = prediction_error_plot(y.flatten(), y_hat.flatten())\n",
    "#         time_str = dataset.sarah.time.dt.strftime('%Y%m%d_%H%M').item()\n",
    "#         fig.savefig(folder + f'prediction_error_plot_{time_str}.png')\n",
    "    except Exception as e:\n",
    "        print(dt)\n",
    "        print(traceback.format_exc())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56cf03f-b589-4882-894f-75bf7232391e",
   "metadata": {},
   "source": [
    "# All-sky vs Clear-sky retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20b088-980c-4163-8809-1ae17b10df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsrn_datasets_cloudy = [GroundstationDataset(f'../../ZARR/IEA_PVPS/IEA_PVPS_{x}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True,\n",
    "                                     filter_csi=('st', .8))\n",
    "                            for x in tqdm(station_names_bsrn)]\n",
    "\n",
    "bsrn_datasets_clearsky = [GroundstationDataset(f'../../ZARR/IEA_PVPS/IEA_PVPS_{x}.zarr', \n",
    "                                        config.y_vars, config.x_vars, config.x_features, config.patch_size['x'], \n",
    "                                        config.transform, config.target_transform, sarah_idx_only=True,\n",
    "                                     filter_csi=('gt', .8))\n",
    "                            for x in tqdm(station_names_bsrn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d30f83-1355-484b-819a-043cbfc7372b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_bsrn_clearsky = predictions_to_ds(station_names_bsrn, bsrn_datasets_clearsky)\n",
    "predictions_bsrn_cloudy = predictions_to_ds(station_names_bsrn, bsrn_datasets_cloudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016d606-4a3c-4a82-93b9-a4864fa4e27a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gt .8\n",
    "print(chkpt_fn)\n",
    "statistics_stations(predictions_bsrn_clearsky, save=True, stations_collection='IEA_PVPS_clearsky_08', plot_text=False)\n",
    "plt.show()\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720594de-9999-4998-bd55-61d1b88f4939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# st .8\n",
    "statistics_stations(predictions_bsrn_cloudy, save=True, stations_collection='IEA_PVPS_cloudy_08', plot_text=False)\n",
    "plt.show()\n",
    "# plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning-kernel2",
   "language": "python",
   "name": "lightning-kernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
